import streamlit as st
from glob import glob
import os
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "mimic-chatbot-75c4155f036a.json"

# ì•± ì œëª© ì„¤ì •
st.set_page_config(page_title="MIMIC QA Chat Bot", page_icon="ğŸ“š")
st.title("ğŸ“š MIMIC QA Chat Bot")
st.markdown("Ask anything about MIMIC dataset (e.g., length of stay, discharge prediction, etc.)")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# í˜„ì¬ ë””ë ‰í† ë¦¬ì— db í´ë” ìƒì„±
DB_DIR = os.path.join(os.getcwd(), "chroma_db")
os.makedirs(DB_DIR, exist_ok=True)

# ì´ˆê¸°í™” í•¨ìˆ˜
@st.cache_resource
def initialize_qa_system():
    # ë¬¸ì„œ ë¡œë”©
    folder_path = "../docs/*/*.pdf"
    pdf_files = glob(folder_path)
    
    if not pdf_files:
        st.error(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        st.stop()
    
    st.info(f"{len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
    
    all_docs = []
    for file in pdf_files:
        try:
            loader = PyPDFLoader(file)
            docs = loader.load()
            all_docs.extend(docs)
        except Exception as e:
            st.warning(f"íŒŒì¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {file} - {e}")

    folder_path = "../docs/*/*.docx"
    word_files = glob(folder_path)
    
    if not word_files:
        st.error(f"WORD íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        st.stop()
    
    st.info(f"{len(word_files)}ê°œì˜ WORD íŒŒì¼ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
    
    for file in word_files:
        try:
            loader = Docx2txtLoader(file)
            docs = loader.load()
            all_docs.extend(docs)
        except Exception as e:
            st.warning(f"íŒŒì¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {file} - {e}")

    folder_path = "../docs/*/*.txt"
    word_files = glob(folder_path)
    
    if not word_files:
        st.error(f"TXT íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        st.stop()
    
    st.info(f"{len(word_files)}ê°œì˜ TXT íŒŒì¼ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
    
    for file in word_files:
        try:
            loader = TextLoader(file)
            docs = loader.load()
            all_docs.extend(docs)
        except Exception as e:
            st.warning(f"íŒŒì¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {file} - {e}")
    
    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    split_docs = text_splitter.split_documents(all_docs)
    
    # ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    try:
        st.info("ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")

        # ëª¨ë¸ ì„¤ì •
        GOOGLE_API_KEY = '****'
        genai.configure(api_key=GOOGLE_API_KEY)

        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        
        # Chroma ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • - ëª…ì‹œì ìœ¼ë¡œ persist_directory ì§€ì •
        vector_store = Chroma.from_documents(split_docs, embeddings, persist_directory=DB_DIR)
        
        # ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        retriever = vector_store.as_retriever(search_kwargs={"k": 10})
        
        model = genai.GenerativeModel(model_name="gemini-2.0-flash")
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        rag_prompt = ChatPromptTemplate.from_template("""
        You are a knowledgeable assistant specializing in clinical data analysis using the MIMIC (Medical Information Mart for Intensive Care) dataset.
        MIMIC is a large, freely-available database comprising de-identified health-related data associated with patients who stayed in critical care units of the Beth Israel Deaconess Medical Center. It includes structured data (e.g., demographics, lab results, medications, procedures, vitals) as well as unstructured data (e.g., clinical notes, discharge summaries).

        Your task is to answer questions based on the retrieved academic context and the conversation history, with a focus on topics involving the MIMIC dataset, such as patient outcomes, length of stay, mortality prediction, readmission, discharge timing, and machine learning applications.

        Use the following pieces of retrieved context and the conversation history to answer the question. 
        If the answer cannot be found in the context or previous history, respond that you don't know rather than making up an answer.

        <context>
        {context}
        </context>

        <conversation_history>
        {history}
        </conversation_history>

        User: {question}
        Assistant:""")
        
        # ë‹µë³€ ìƒì„± í•¨ìˆ˜
        def answer_question(question, history):
            # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë° í¬ë§·íŒ…
            context = retriever.invoke(question)
            formatted_context = "\n\n".join(doc.page_content for doc in context)
            
            # ì‘ë‹µ ìƒì„±
            response = model.generate_content(
                rag_prompt.format(
                    context=formatted_context,
                    history=history,
                    question=question
                )
            )
            
            return response.text
        
        return answer_question
        
    except Exception as e:
        import traceback
        st.error(f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.code(traceback.format_exc())
        return None

# ì´ˆê¸°í™” ìƒíƒœ í‘œì‹œ
with st.spinner('ğŸ“„ ë¬¸ì„œë“¤ì„ ë¡œë”©í•˜ê³  ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...'):
    qa_function = initialize_qa_system()
    
    if qa_function:
        st.success("âœ… ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        st.error("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

# ì €ì¥ëœ ëŒ€í™” í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ëŒ€í™” ê¸°ë¡ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    history_text = ""
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            history_text += f"User: {msg['content']}\n"
        else:
            history_text += f"Assistant: {msg['content']}\n"
    
    # ë´‡ ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
    with st.chat_message("assistant"):
        with st.spinner("ìƒê° ì¤‘..."):
            response = qa_function(prompt, history_text)
            st.markdown(response)
    
    # ë´‡ ë©”ì‹œì§€ ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": response})

# ì‚¬ì´ë“œë°”ì— ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
with st.sidebar:
    if st.button("ğŸ§¹ ëŒ€í™” ë‚´ìš© ì§€ìš°ê¸°"):
        st.session_state.messages = []
        st.rerun()
    
    st.markdown("---")
    st.markdown("### ëª¨ë¸ ì •ë³´")
    st.markdown("- ì‚¬ìš© ëª¨ë¸: `gemini-2.0-flash`")
    st.markdown("- ì„ë² ë”©: `gemini/embedding-001`")

# streamlit run app_gemini.py
