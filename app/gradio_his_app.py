import gradio as gr
from glob import glob
from langchain_community.document_loaders import PyPDFLoader
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ë¬¸ì„œ ë¡œë”©
def load_documents(folder_path="../DY_Applied/*.pdf"):
    pdf_files = glob(folder_path)
    all_docs = []
    for file in pdf_files:
        loader = PyPDFLoader(file)
        docs = loader.load_and_split()
        all_docs.extend(docs)
    return all_docs

# VectorStore ìƒì„±
def build_vectorstore(docs):
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)
    return vectorstore

# QA Chain ìƒì„±
def build_qa_chain(vectorstore):
    retriever = vectorstore.as_retriever()
    model = ChatOllama(
        model="gemma3:12b"  # ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½ ê°€ëŠ¥
    )

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    rag_prompt = ChatPromptTemplate.from_template("""
    You are an assistant for question-answering tasks. 
    Use the following pieces of retrieved context and the conversation history to answer the question.
    If you don't know the answer, just say that you don't know.

    <context>
    {context}
    </context>

    <conversation_history>
    {history}
    </conversation_history>

    User: {question}
    Assistant:""")

    def run_chain(inputs):
        context = retriever.invoke(inputs["question"])
        formatted_context = format_docs(context)
        
        response = model.invoke(
            rag_prompt.format(
                context=formatted_context,
                history=inputs["history"],
                question=inputs["question"]
            )
        )
        
        return StrOutputParser().invoke(response)

    return run_chain

# ì´ˆê¸°í™”
print("ğŸ“„ Loading PDFs and creating vectorstore...")
docs = load_documents()
vectorstore = build_vectorstore(docs)
qa_chain = build_qa_chain(vectorstore)
print("âœ… Ready to serve!")

# Gradio ì¸í„°í˜ì´ìŠ¤ (ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¡œ ë³€ê²½)
def respond(message, chat_history):
    # íˆìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    history_text = ""
    for user_msg, bot_msg in chat_history:
        history_text += f"User: {user_msg}\nAssistant: {bot_msg}\n"
    
    # ì§ˆë¬¸ê³¼ íˆìŠ¤í† ë¦¬ë¥¼ ì²´ì¸ì— ì „ë‹¬
    bot_response = qa_chain({"question": message, "history": history_text})
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ìƒˆ ëŒ€í™” ì¶”ê°€ (Gradio Chatbot í˜•ì‹ì— ë§ê²Œ)
    chat_history.append((message, bot_response))
    
    # ìƒˆ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•œ í›„ í…ìŠ¤íŠ¸ë°•ìŠ¤ ë¹„ìš°ê¸° ìœ„í•œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    return "", chat_history

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
with gr.Blocks() as demo:
    gr.Markdown("# ğŸ“š MIMIC PDF QA Bot with Memory")
    gr.Markdown("Ask about papers that used MIMIC dataset (e.g., length of stay, discharge prediction, etc.)")
    
    chatbot = gr.Chatbot(height=500)
    msg = gr.Textbox(label="ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
    clear = gr.Button("ğŸ§¹ ëŒ€í™” ë‚´ìš© ì§€ìš°ê¸°")
    
    # ì‘ë‹µ í•¨ìˆ˜ì˜ ì¶œë ¥ì„ msgì™€ chatbot ëª¨ë‘ì— ì—°ê²°
    msg.submit(respond, [msg, chatbot], [msg, chatbot])
    
    # ëŒ€í™” ë‚´ìš© ì´ˆê¸°í™”
    clear.click(lambda: [], None, chatbot, queue=False)

if __name__ == "__main__":
    demo.launch(server_port=7860)